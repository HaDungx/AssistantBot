{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\chat\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in GPT4AllEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\chat\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader,Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_loader(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.pdf':\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif ext == '.csv':\n",
    "        return CSVLoader(file_path)\n",
    "    elif ext == '.docx' or ext == '.doc':\n",
    "        return Docx2txtLoader(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_all_files(directory_path):\n",
    "    combined_text = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            loader = select_loader(file_path)\n",
    "            if loader:\n",
    "                documents = loader.load()\n",
    "                combined_text += documents\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "def crawl_web(urls):\n",
    "    combined_text = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Loại bỏ các phần không cần thiết như script, style\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            text = soup.get_text(separator='\\n')\n",
    "            combined_text.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Không thể crawl {url}: {e}\")\n",
    "    return combined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data\"\n",
    "urls_to_crawl = [\n",
    "    \"https://thptbuithixuan.hcm.edu.vn/homegd3\",\n",
    "]\n",
    "web_documents = crawl_web(urls_to_crawl)\n",
    "documents = load_all_files(directory_path)\n",
    "processed_web_documents = [Document(page_content=doc, metadata={}) for doc in web_documents]\n",
    "\n",
    "\n",
    "combined_documents = processed_web_documents + documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9488\\2623411752.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\chat\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\chat\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(combined_documents)\n",
    "\n",
    "# Embeding\n",
    "# embedding_model = GPT4AllEmbeddings(model_file=\"models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "db = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatGroq(temperature=0, model=\"llama3-8b-8192\")\n",
    "# model_file = 'vinallama-7b-chat_q5_0.gguf'\n",
    "# llm = CTransformers(\n",
    "#         model=model_file,\n",
    "#         model_type=\"llama\",\n",
    "#         max_new_tokens=1024,\n",
    "#         temperature=0.01\n",
    "#     )\n",
    "\n",
    "# prompt = \"Bạn hãy giới thiệu về bản thân\"\n",
    "# output = llm(prompt, max_tokens=50, temperature=0.5)\n",
    "\n",
    "\n",
    "# In kết quả\n",
    "# print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|im_start|>system\\nSử dụng thông tin sau đây để trả lời câu hỏi. Nếu bạn không biết câu trả lời, hãy nói tôi không biết, đừng cố tạo ra câu trả lời\\n\n",
    "    {context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\"\"\"\n",
    "prompt = PromptTemplate(template = template, input_variables=[\"context\", \"question\"])\n",
    "llm_chain = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type= \"stuff\",\n",
    "        # retriever = db.as_retriever(search_kwargs = {\"k\":3}, max_tokens_limit=1024),\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": 3, \"fuzzy\": True}, max_tokens_limit=1024),\n",
    "\n",
    "        return_source_documents = False,\n",
    "        chain_type_kwargs= {'prompt': prompt}\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thầy Huỳnh Thanh Phú.\n"
     ]
    }
   ],
   "source": [
    "question = \"hiệu trưởng trường Bùi Thị Xuân là ai\" + \"trả lời tiếng việt\"\n",
    "response = llm_chain.invoke({\"query\": question})\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
